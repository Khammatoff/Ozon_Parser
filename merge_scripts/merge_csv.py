import pandas as pd
import glob
import os
import sys
from datetime import datetime


def merge_csv_files():
    """–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö CSV —Ñ–∞–π–ª–æ–≤ –≤ –æ–¥–∏–Ω —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–ª–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    data_dir = "/app/data"
    output_file = f"{data_dir}/combined_sellers_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"

    print("üîç –ü–æ–∏—Å–∫ CSV —Ñ–∞–π–ª–æ–≤...")

    # –ò—â–µ–º –≤—Å–µ CSV —Ñ–∞–π–ª—ã
    csv_files = glob.glob(f"{data_dir}/sellers_*.csv")

    if not csv_files:
        print("‚ùå CSV —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        return

    print(f"üìÅ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(csv_files)}")

    # –ß–∏—Ç–∞–µ–º –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã
    dataframes = []
    for file in csv_files:
        try:
            df = pd.read_csv(file, encoding='utf-8-sig')
            dataframes.append(df)
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω: {os.path.basename(file)} ({len(df)} —Å—Ç—Ä–æ–∫)")

            # –í—ã–≤–æ–¥–∏–º –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            print(f"   –ö–æ–ª–æ–Ω–∫–∏: {list(df.columns)}")

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {file}: {e}")

    if not dataframes:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞")
        return

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ DataFrame
    combined_df = pd.concat(dataframes, ignore_index=True)

    before_dedup = len(combined_df)
    print(f"üìä –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –¥–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {before_dedup}")
    print(f"üìã –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {list(combined_df.columns)}")

    # üî• –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫ –∏–∑ –ø–∞—Ä—Å–µ—Ä–∞
    def get_data_completeness_score(row):
        """–û—Ü–µ–Ω–∏–≤–∞–µ–º –ø–æ–ª–Ω–æ—Ç—É –¥–∞–Ω–Ω—ã—Ö –≤ —Å—Ç—Ä–æ–∫–µ"""
        score = 0

        # üî• –ü–†–ê–í–ò–õ–¨–ù–´–ï –ù–ê–ó–í–ê–ù–ò–Ø –ö–û–õ–û–ù–û–ö (–∏–∑ –≤–∞—à–µ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞):
        # 'URL', '–Ω–∞–∑–≤–∞–Ω–∏–µ', 'Html', '–û–ì–†–ù', '–ò–ù–ù', '–ù–∞–∑–≤–∞–Ω–∏–µ —é—Ä –ª–∏—Ü–∞',
        # '–ö–æ–ª-–≤–æ –æ—Ç–∑—ã–≤–æ–≤', '—Ä–µ–π—Ç–∏–Ω–≥', '–°—Ä–æ–∫ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏', '–¢–æ–≤–∞—Ä—ã'

        # –î–∞–µ–º –±–æ–ª—å—à–∏–π –≤–µ—Å –≤–∞–∂–Ω—ã–º –ø–æ–ª—è–º
        if pd.notna(row.get('–û–ì–†–ù')) and row.get('–û–ì–†–ù') != '':
            score += 10
        if pd.notna(row.get('–ù–∞–∑–≤–∞–Ω–∏–µ —é—Ä –ª–∏—Ü–∞')) and row.get('–ù–∞–∑–≤–∞–Ω–∏–µ —é—Ä –ª–∏—Ü–∞') != '':
            score += 8
        if pd.notna(row.get('—Ä–µ–π—Ç–∏–Ω–≥')) and row.get('—Ä–µ–π—Ç–∏–Ω–≥') != '':
            score += 5
        if pd.notna(row.get('–ö–æ–ª-–≤–æ –æ—Ç–∑—ã–≤–æ–≤')) and row.get('–ö–æ–ª-–≤–æ –æ—Ç–∑—ã–≤–æ–≤') != '':
            score += 5
        if pd.notna(row.get('–°—Ä–æ–∫ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏')) and row.get('–°—Ä–æ–∫ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏') != '':
            score += 5
        if pd.notna(row.get('–ò–ù–ù')) and row.get('–ò–ù–ù') != '':
            score += 7

        # –£—á–∏—Ç—ã–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö –ø–æ–ª–µ–π
        filled_fields = sum(1 for value in row.values if pd.notna(value) and value != '')
        score += filled_fields
        return score

    # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–æ–ª–±–µ—Ü —Å –æ—Ü–µ–Ω–∫–æ–π –ø–æ–ª–Ω–æ—Ç—ã –¥–∞–Ω–Ω—ã—Ö
    combined_df['_completeness_score'] = combined_df.apply(get_data_completeness_score, axis=1)

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø–æ–ª–Ω–æ—Ç–µ –¥–∞–Ω–Ω—ã—Ö (—Å–∞–º—ã–µ –ø–æ–ª–Ω—ã–µ –∑–∞–ø–∏—Å–∏ - –ø–µ—Ä–≤—ã–º–∏)
    combined_df = combined_df.sort_values('_completeness_score', ascending=False)

    # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ URL, –æ—Å—Ç–∞–≤–ª—è—è —Å–∞–º—ã–µ –ø–æ–ª–Ω—ã–µ –∑–∞–ø–∏—Å–∏
    combined_df = combined_df.drop_duplicates(subset=['URL'], keep='first')

    # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Å—Ç–æ–ª–±–µ—Ü
    combined_df = combined_df.drop('_completeness_score', axis=1)

    after_dedup = len(combined_df)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
    combined_df.to_csv(output_file, index=False, encoding='utf-8-sig')

    print(f"üéâ –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_file}")
    print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"   - –ò—Å—Ö–æ–¥–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {len(csv_files)}")
    print(f"   - –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –¥–æ —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {before_dedup}")
    print(f"   - –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {after_dedup}")
    print(f"   - –£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {before_dedup - after_dedup}")

    # üî• –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê –ö–ê–ß–ï–°–¢–í–ê –î–ê–ù–ù–´–•
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–æ–ª–æ–Ω–æ–∫ –ø–µ—Ä–µ–¥ –∞–Ω–∞–ª–∏–∑–æ–º
        has_ogrn = '–û–ì–†–ù' in combined_df.columns
        has_legal_name = '–ù–∞–∑–≤–∞–Ω–∏–µ —é—Ä –ª–∏—Ü–∞' in combined_df.columns

        if has_ogrn and has_legal_name:
            complete_records = len(combined_df[
                                       (combined_df['–û–ì–†–ù'].notna()) &
                                       (combined_df['–û–ì–†–ù'] != '') &
                                       (combined_df['–ù–∞–∑–≤–∞–Ω–∏–µ —é—Ä –ª–∏—Ü–∞'].notna()) &
                                       (combined_df['–ù–∞–∑–≤–∞–Ω–∏–µ —é—Ä –ª–∏—Ü–∞'] != '')
                                       ])
            print(f"   - –ó–∞–ø–∏—Å–µ–π —Å –ø–æ–ª–Ω—ã–º–∏ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏: {complete_records}")
        else:
            print(f"   - –Æ—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ: –∫–æ–ª–æ–Ω–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")

    except Exception as e:
        print(f"   - –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö: {e}")

    # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
    stats_file = f"{data_dir}/merge_stats_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
    with open(stats_file, 'w', encoding='utf-8') as f:
        f.write("–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è CSV\n")
        f.write("=" * 40 + "\n")
        f.write(f"–î–∞—Ç–∞: {datetime.now()}\n")
        f.write(f"–ò—Å—Ö–æ–¥–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {len(csv_files)}\n")
        f.write(f"–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –¥–æ —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {before_dedup}\n")
        f.write(f"–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {after_dedup}\n")
        f.write(f"–£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {before_dedup - after_dedup}\n")

        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–ª–æ–Ω–∫–∞—Ö
        f.write(f"–ö–æ–ª–æ–Ω–∫–∏ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ: {list(combined_df.columns)}\n\n")
        f.write("–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:\n")
        for file in csv_files:
            f.write(f"- {os.path.basename(file)}\n")

    print(f"üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {stats_file}")

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
    print("\nüîç –ü—Ä–∏–º–µ—Ä –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:")
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        available_columns = ['URL', '–Ω–∞–∑–≤–∞–Ω–∏–µ', '–û–ì–†–ù', '–ù–∞–∑–≤–∞–Ω–∏–µ —é—Ä –ª–∏—Ü–∞']
        display_columns = [col for col in available_columns if col in combined_df.columns]

        if display_columns:
            sample_data = combined_df[display_columns].head(3)
            for _, row in sample_data.iterrows():
                has_ogrn = pd.notna(row.get('–û–ì–†–ù')) and row.get('–û–ì–†–ù') != '' if '–û–ì–†–ù' in row else False
                status = "‚úÖ –ü–û–õ–ù–´–ï –î–ê–ù–ù–´–ï" if has_ogrn else "‚ùå –ù–ï–ü–û–õ–ù–´–ï –î–ê–ù–ù–´–ï"
                name = row.get('–Ω–∞–∑–≤–∞–Ω–∏–µ', '–Ω–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏—è') if '–Ω–∞–∑–≤–∞–Ω–∏–µ' in row else '–Ω–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏—è'
                print(f"   {status} | {row['URL']} | {name}")
        else:
            print("   –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è")

    except Exception as e:
        print(f"   –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–≤–æ–¥–µ –ø—Ä–∏–º–µ—Ä–∞: {e}")


if __name__ == "__main__":
    merge_csv_files()